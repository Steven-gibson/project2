{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8c2fc8-37f9-4fe2-9cfc-97cf03779714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 15:54:47.076319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Visualize training history\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48d7416-f48b-4aed-9c95-bbca2ad99ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (5000, 17)\n",
      "y (5000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48</td>\n",
       "      <td>1052.255952</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.013043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>437.391304</td>\n",
       "      <td>2</td>\n",
       "      <td>235.55</td>\n",
       "      <td>83</td>\n",
       "      <td>2503.881781</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>2.086218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>41.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>126</td>\n",
       "      <td>4310.004668</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>3.451072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>606.666667</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.026389</td>\n",
       "      <td>36.672294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>608.140000</td>\n",
       "      <td>6</td>\n",
       "      <td>733.80</td>\n",
       "      <td>168</td>\n",
       "      <td>4948.398759</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>10.150644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>415.250000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14</td>\n",
       "      <td>186.933333</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>149.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55</td>\n",
       "      <td>2598.991667</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>48.729956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>140.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>88.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               3               142.500000              0   \n",
       "1               6               437.391304              2   \n",
       "2               1                41.125000              0   \n",
       "3               2               141.000000              0   \n",
       "4              18               608.140000              6   \n",
       "5               1                22.000000              0   \n",
       "6               0                 0.000000              0   \n",
       "7               0                 0.000000              0   \n",
       "8               8               149.500000              0   \n",
       "9               6               140.333333              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                    0.00              48              1052.255952   \n",
       "1                  235.55              83              2503.881781   \n",
       "2                    0.00             126              4310.004668   \n",
       "3                    0.00              10               606.666667   \n",
       "4                  733.80             168              4948.398759   \n",
       "5                    0.00               9               415.250000   \n",
       "6                    0.00              14               186.933333   \n",
       "7                    0.00              12               198.000000   \n",
       "8                    0.00              55              2598.991667   \n",
       "9                    0.00               9                88.950000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay  Month  OperatingSystems  \\\n",
       "0     0.004348   0.013043    0.000000         0.0     11                 1   \n",
       "1     0.002198   0.004916    2.086218         0.0      3                 2   \n",
       "2     0.000688   0.012823    3.451072         0.0     11                 2   \n",
       "3     0.008333   0.026389   36.672294         0.0      8                 2   \n",
       "4     0.006632   0.013528   10.150644         0.0      8                 2   \n",
       "5     0.033333   0.048148    0.000000         0.0      3                 3   \n",
       "6     0.042857   0.071429    0.000000         0.0      5                 2   \n",
       "7     0.016667   0.075000    0.000000         0.0      3                 2   \n",
       "8     0.003279   0.008197   48.729956         0.0      5                 2   \n",
       "9     0.000000   0.004762    0.000000         0.0      5                 3   \n",
       "\n",
       "   Browser  Region  TrafficType  VisitorType  Weekend  Revenue  \n",
       "0        8       6           11            1        0        0  \n",
       "1        2       3            2            1        0        1  \n",
       "2        2       2            2            1        0        0  \n",
       "3        5       7            4            1        0        0  \n",
       "4        2       3            1            1        1        0  \n",
       "5        3       1            1            1        0        0  \n",
       "6        2       3            4            1        0        0  \n",
       "7        2       3            2            1        0        0  \n",
       "8        4       8            2            1        1        0  \n",
       "9        2       2            3            2        0        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"Project1_data/shopping.csv\"\n",
    "#file_path = \"unseen.csv\"\n",
    "rf = pd.read_csv(file_path)\n",
    "#mapping for replacement\n",
    "Month_mapping = {\n",
    "    \"Jan\":1,\n",
    "    \"Feb\":2,\n",
    "    \"Mar\":3,\n",
    "    \"Apr\":4,\n",
    "    \"May\":5,\n",
    "    \"June\":6,\n",
    "    \"Jul\":7,\n",
    "    \"Aug\":8,\n",
    "    \"Sep\":9,\n",
    "    \"Oct\":10,\n",
    "    \"Nov\":11,\n",
    "    \"Dec\":12\n",
    "    }\n",
    "\n",
    "Visitor_mapping = {\n",
    "    \"Returning_Visitor\": 1,\n",
    "    \"New_Visitor\": 2,\n",
    "    \"Other\": 3\n",
    "}\n",
    "\n",
    "Bool_mapping = {\n",
    "    True: 1,\n",
    "    False: 0\n",
    "}\n",
    "#Month(1-12) & VisitorType(1,2,3), Weekend & Revenue(true,false/1,0)\n",
    "#replace\n",
    "rf['Month'] = rf['Month'].replace(Month_mapping)\n",
    "rf['VisitorType']=rf['VisitorType'].replace(Visitor_mapping)\n",
    "rf['Weekend'] = rf['Weekend'].replace(Bool_mapping)\n",
    "rf['Revenue'] = rf['Revenue'].replace(Bool_mapping)\n",
    "#puts data into numpy array\n",
    "data_arr = rf.to_numpy()\n",
    "x = data_arr[:5000,:17]\n",
    "y = data_arr[:5000,17]\n",
    "#shows different info types\n",
    "#rf.dtypes\n",
    "#prints shapes\n",
    "print(\"X\",x.shape)\n",
    "print(\"y\",y.shape)\n",
    "rf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa820f94-58e8-45ac-a8aa-7863aa5d6813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (5000,)\n",
      " (5000, 17)\n",
      " [ 0.21173582  0.35206422 -0.38787235 -0.23716731  0.35785985 -0.06903133\n",
      " -0.3764785  -0.61938805 -0.31442715 -0.31311801  0.99578857 -1.22770075\n",
      "  3.26250963  1.17721659  1.71636354 -0.39932615 -0.55763754]\n"
     ]
    }
   ],
   "source": [
    "#create Scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#scale the features\n",
    "X = scaler.fit_transform(x)\n",
    "\n",
    "print(\"\",y.shape)\n",
    "print(\"\",X.shape)\n",
    "print(\"\",X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad81fd70-8308-4658-9b8e-e3c35e075caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid function\n",
    "def sigmoid(z):\n",
    "    g = 1 / (1 + math.e**-z)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7e5973-68d2-46ce-b01b-66c8ed41aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(feature_data):\n",
    "    #run feature data through the trained model to get the linera activations from the output layer\n",
    "    logits = model(feature_data)\n",
    "\n",
    "    #convert the model outputs to probabilities by running through the sigmoid function\n",
    "    logits = sigmoid(logits)\n",
    "\n",
    "    # get predictions by converting output probabilities to True if >= 0.5, and False if < 0.5\n",
    "    predictions = logits >= 0.5\n",
    "\n",
    "    #convert True to 1 and False to 0\n",
    "    predictions = [int(boolean) for boolean in predictions]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa808fb-e40a-495a-b907-314e7296d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(labels, predictions):\n",
    "    \n",
    "    # Calculate the number of correct predictions by comparing 'labels' and 'predictions'\n",
    "    correct_predictions = np.sum(labels == predictions)\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(labels)\n",
    "    \n",
    "    # Calculate the accuracy as a percentage\n",
    "    accuracy_percentage = (correct_predictions / total_predictions) * 100.0\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19893745-48be-4ccd-8f9b-85c5f200348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(true_labels, predictions):\n",
    "    #plot confusion matrix\n",
    "    #mat = confusion_matrix(true_labels, predictions)\n",
    "    #plot_confusion_matrix(conf_mat=mat)\n",
    "\n",
    "    #Calculate precision, recall, f1_score\n",
    "\n",
    "    #precision\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "\n",
    "    #recall\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "\n",
    "    #fl score\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "    #Print precision, recall, f1_score\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6212be9-55b8-4c98-a707-d446ddee7b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3000, 17)\n",
      "X_test shape: (1000, 17)\n",
      "X_cv shape: (1000, 17)\n",
      "y_train shape: (3000,)\n",
      "y_test shape: (1000,)\n",
      "y_cv shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# split  data into training, CV, test sets\n",
    "# use the train_test_split modules\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=35)\n",
    "\n",
    "#split test set 50:50 for the CV and test sets\n",
    "X_test, X_cv, y_test, y_cv = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "#Print the shapes of the resulting sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_cv shape: {X_cv.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"y_cv shape: {y_cv.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a98bc33-e825-406f-b717-88907c45d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_70 (Dense)            (None, 128)               2304      \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12673 (49.50 KB)\n",
      "Trainable params: 12673 (49.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(17,)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.3)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.1)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a074df9-77be-405c-b50a-cdac401785dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "94/94 [==============================] - 1s 3ms/step - loss: 7.5801 - accuracy: 0.8453 - val_loss: 3.4860 - val_accuracy: 0.8520\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.9503 - accuracy: 0.8527 - val_loss: 0.9772 - val_accuracy: 0.8570\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.6613 - accuracy: 0.8573 - val_loss: 0.4585 - val_accuracy: 0.8650\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8670 - val_loss: 0.3714 - val_accuracy: 0.8760\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8720 - val_loss: 0.3474 - val_accuracy: 0.8750\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.8770 - val_loss: 0.3381 - val_accuracy: 0.8750\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8743 - val_loss: 0.3435 - val_accuracy: 0.8800\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8787 - val_loss: 0.3374 - val_accuracy: 0.8780\n",
      "Epoch 9/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8763 - val_loss: 0.3309 - val_accuracy: 0.8810\n",
      "Epoch 10/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8797 - val_loss: 0.3243 - val_accuracy: 0.8810\n",
      "Epoch 11/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8800 - val_loss: 0.3303 - val_accuracy: 0.8820\n",
      "Epoch 12/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.8750 - val_loss: 0.3204 - val_accuracy: 0.8770\n",
      "Epoch 13/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8777 - val_loss: 0.3183 - val_accuracy: 0.8810\n",
      "Epoch 14/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8803 - val_loss: 0.3343 - val_accuracy: 0.8840\n",
      "Epoch 15/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8790 - val_loss: 0.3315 - val_accuracy: 0.8740\n",
      "Epoch 16/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8800 - val_loss: 0.3082 - val_accuracy: 0.8770\n",
      "Epoch 17/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8797 - val_loss: 0.3091 - val_accuracy: 0.8850\n",
      "Epoch 18/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8807 - val_loss: 0.3246 - val_accuracy: 0.8820\n",
      "Epoch 19/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8833 - val_loss: 0.3122 - val_accuracy: 0.8800\n",
      "Epoch 20/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8787 - val_loss: 0.3255 - val_accuracy: 0.8820\n",
      "Epoch 21/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8837 - val_loss: 0.3217 - val_accuracy: 0.8770\n",
      "Epoch 22/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8823 - val_loss: 0.3293 - val_accuracy: 0.8800\n",
      "Epoch 23/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8800 - val_loss: 0.3088 - val_accuracy: 0.8840\n",
      "Epoch 24/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8817 - val_loss: 0.3126 - val_accuracy: 0.8860\n",
      "Epoch 25/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8800 - val_loss: 0.3257 - val_accuracy: 0.8830\n",
      "Epoch 26/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8857 - val_loss: 0.3200 - val_accuracy: 0.8800\n",
      "Epoch 27/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8793 - val_loss: 0.3215 - val_accuracy: 0.8850\n",
      "Epoch 28/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8787 - val_loss: 0.3145 - val_accuracy: 0.8780\n",
      "Epoch 29/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8830 - val_loss: 0.3221 - val_accuracy: 0.8800\n",
      "Epoch 30/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8817 - val_loss: 0.3133 - val_accuracy: 0.8840\n",
      "Epoch 31/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8837 - val_loss: 0.3120 - val_accuracy: 0.8820\n",
      "Epoch 32/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8837 - val_loss: 0.3117 - val_accuracy: 0.8860\n",
      "Epoch 33/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3168 - accuracy: 0.8813 - val_loss: 0.3087 - val_accuracy: 0.8840\n",
      "Epoch 34/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8823 - val_loss: 0.3132 - val_accuracy: 0.8740\n",
      "Epoch 35/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.8817 - val_loss: 0.3037 - val_accuracy: 0.8840\n",
      "Epoch 36/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8817 - val_loss: 0.3071 - val_accuracy: 0.8890\n",
      "Epoch 37/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8817 - val_loss: 0.3071 - val_accuracy: 0.8810\n",
      "Epoch 38/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.8853 - val_loss: 0.3070 - val_accuracy: 0.8910\n",
      "Epoch 39/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8837 - val_loss: 0.3080 - val_accuracy: 0.8800\n",
      "Epoch 40/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8857 - val_loss: 0.3060 - val_accuracy: 0.8840\n",
      "Epoch 41/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8853 - val_loss: 0.3104 - val_accuracy: 0.8810\n",
      "Epoch 42/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8797 - val_loss: 0.2960 - val_accuracy: 0.8840\n",
      "Epoch 43/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3103 - accuracy: 0.8840 - val_loss: 0.3090 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8853 - val_loss: 0.3078 - val_accuracy: 0.8820\n",
      "Epoch 45/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8820 - val_loss: 0.2960 - val_accuracy: 0.8870\n",
      "Epoch 46/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8843 - val_loss: 0.3158 - val_accuracy: 0.8710\n",
      "Epoch 47/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8840 - val_loss: 0.3241 - val_accuracy: 0.8890\n",
      "Epoch 48/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3024 - accuracy: 0.8837 - val_loss: 0.2975 - val_accuracy: 0.8840\n",
      "Epoch 49/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.8853 - val_loss: 0.3188 - val_accuracy: 0.8810\n",
      "Epoch 50/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8857 - val_loss: 0.3140 - val_accuracy: 0.8870\n",
      "Epoch 51/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3035 - accuracy: 0.8797 - val_loss: 0.2982 - val_accuracy: 0.8820\n",
      "Epoch 52/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8863 - val_loss: 0.3117 - val_accuracy: 0.8930\n",
      "Epoch 53/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8817 - val_loss: 0.3090 - val_accuracy: 0.8830\n",
      "Epoch 54/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8843 - val_loss: 0.3095 - val_accuracy: 0.8920\n",
      "Epoch 55/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8857 - val_loss: 0.2972 - val_accuracy: 0.8900\n",
      "Epoch 56/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8840 - val_loss: 0.2954 - val_accuracy: 0.8770\n",
      "Epoch 57/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8840 - val_loss: 0.3018 - val_accuracy: 0.8880\n",
      "Epoch 58/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8833 - val_loss: 0.2989 - val_accuracy: 0.8830\n",
      "Epoch 59/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8857 - val_loss: 0.2938 - val_accuracy: 0.8820\n",
      "Epoch 60/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8837 - val_loss: 0.3001 - val_accuracy: 0.8870\n",
      "Epoch 61/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8843 - val_loss: 0.3199 - val_accuracy: 0.8920\n",
      "Epoch 62/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8870 - val_loss: 0.2943 - val_accuracy: 0.8770\n",
      "Epoch 63/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8837 - val_loss: 0.2954 - val_accuracy: 0.8860\n",
      "Epoch 64/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8833 - val_loss: 0.2979 - val_accuracy: 0.8950\n",
      "Epoch 65/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8810 - val_loss: 0.3036 - val_accuracy: 0.8890\n",
      "Epoch 66/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8857 - val_loss: 0.2894 - val_accuracy: 0.8890\n",
      "Epoch 67/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.8840 - val_loss: 0.3017 - val_accuracy: 0.8870\n",
      "Epoch 68/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.8830 - val_loss: 0.2979 - val_accuracy: 0.8920\n",
      "Epoch 69/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.8873 - val_loss: 0.2975 - val_accuracy: 0.8870\n",
      "Epoch 70/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8857 - val_loss: 0.2939 - val_accuracy: 0.8880\n",
      "Epoch 71/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8823 - val_loss: 0.2949 - val_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8823 - val_loss: 0.3065 - val_accuracy: 0.8880\n",
      "Epoch 73/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8830 - val_loss: 0.3028 - val_accuracy: 0.8870\n",
      "Epoch 74/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8843 - val_loss: 0.2886 - val_accuracy: 0.8960\n",
      "Epoch 75/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8857 - val_loss: 0.2937 - val_accuracy: 0.8910\n",
      "Epoch 76/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8857 - val_loss: 0.2869 - val_accuracy: 0.8890\n",
      "Epoch 77/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8847 - val_loss: 0.3005 - val_accuracy: 0.8710\n",
      "Epoch 78/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8847 - val_loss: 0.2888 - val_accuracy: 0.8870\n",
      "Epoch 79/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8900 - val_loss: 0.2900 - val_accuracy: 0.8910\n",
      "Epoch 80/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8880 - val_loss: 0.2978 - val_accuracy: 0.8890\n",
      "Epoch 81/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8850 - val_loss: 0.3091 - val_accuracy: 0.8540\n",
      "Epoch 82/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2984 - accuracy: 0.8770 - val_loss: 0.2886 - val_accuracy: 0.8920\n",
      "Epoch 83/100\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8893 - val_loss: 0.2906 - val_accuracy: 0.8850\n",
      "Epoch 84/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8837 - val_loss: 0.2977 - val_accuracy: 0.8940\n",
      "Epoch 85/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8857 - val_loss: 0.2969 - val_accuracy: 0.8820\n",
      "Epoch 86/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8877 - val_loss: 0.2979 - val_accuracy: 0.8850\n",
      "Epoch 87/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.8860 - val_loss: 0.2979 - val_accuracy: 0.8860\n",
      "Epoch 88/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.8887 - val_loss: 0.2899 - val_accuracy: 0.8930\n",
      "Epoch 89/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8840 - val_loss: 0.2882 - val_accuracy: 0.8880\n",
      "Epoch 90/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8860 - val_loss: 0.2999 - val_accuracy: 0.8850\n",
      "Epoch 91/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8847 - val_loss: 0.2912 - val_accuracy: 0.8900\n",
      "Epoch 92/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8840 - val_loss: 0.2839 - val_accuracy: 0.8890\n",
      "Epoch 93/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8890 - val_loss: 0.3003 - val_accuracy: 0.8850\n",
      "Epoch 94/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2908 - accuracy: 0.8870 - val_loss: 0.2908 - val_accuracy: 0.8870\n",
      "Epoch 95/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8857 - val_loss: 0.2888 - val_accuracy: 0.8840\n",
      "Epoch 96/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8863 - val_loss: 0.2992 - val_accuracy: 0.8930\n",
      "Epoch 97/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.8900 - val_loss: 0.3027 - val_accuracy: 0.8900\n",
      "Epoch 98/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8910 - val_loss: 0.2929 - val_accuracy: 0.8800\n",
      "Epoch 99/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8890 - val_loss: 0.2957 - val_accuracy: 0.8870\n",
      "Epoch 100/100\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8843 - val_loss: 0.2980 - val_accuracy: 0.8880\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy']    \n",
    ")\n",
    "\n",
    "#fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_cv, y_cv),\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "593c8245-3ede-4100-b3c5-b1c0f03dba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8880\n",
      "Cross accuracy: 0.8880000114440918\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8850\n",
      "Test accuracy: 0.8849999904632568\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8943\n",
      "Train accuracy: 0.8943333625793457\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_cv, y_cv)\n",
    "print(f'Cross accuracy: {test_acc}')\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "test_loss, test_acc = model.evaluate(X_train, y_train)\n",
    "print(f'Train accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de456377-89c9-43c9-9d0c-b6646388cc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.90%\n",
      "Precision: 0.59\n",
      "Recall: 0.54\n",
      "F1 Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "#results for the test set\n",
    "#Predictions\n",
    "#print(\"Predictions\",test_pred)\n",
    "#Accuracy\n",
    "test_pred = get_predictions(X_test)\n",
    "calculate_accuracy(y_test,test_pred)\n",
    "#Confusion matrix /F1,recall,Precison/\n",
    "print_confusion_matrix(y_test, test_pred)\n",
    "\n",
    "# need help with regularization and lambda and raise F1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e71ffe4-4234-4617-a6fd-3a86bf5d6380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwBUlEQVR4nO3deZxcZZ3v8c+v9t73dJLubEASICEEs8gqMAgEEEFBxRFndBwZRnHQq444XkfmenW46rjjIDooznVAL4hEjajoREBQEjBAVgjZurP2vtf+u388p5NKp7vTnXSlSJ/f+/WqF12nTp36Pd3hfOt5nrOIqmKMMca/AoUuwBhjTGFZEBhjjM9ZEBhjjM9ZEBhjjM9ZEBhjjM9ZEBhjjM9ZEBgzRiLyfRH532Ncd4eIvPF4t2PMiWBBYIwxPmdBYIwxPmdBYCYVb0jm4yLyooj0ich/iEi9iPxSRHpE5HERqcpZ/80iskFEOkVktYickfPaOSLyvPe+HwGxIZ/1JhFZ5733aRFZdIw1v19EtopIu4isFJHp3nIRka+IyAER6fLatNB77WoR2ejVtltEPnZMvzBjsCAwk9MNwOXAPOBa4JfAPwG1uH/z/wAgIvOAB4APA3XAKuBnIhIRkQjwU+A/gWrg/3nbxXvv64D7gL8DaoBvAytFJDqeQkXkL4B/Bd4OTAN2Ag96L18BvMFrRyXwDqDNe+0/gL9T1TJgIfC78XyuMbksCMxk9A1V3a+qu4EngT+p6p9VNQE8ApzjrfcO4Beq+htVTQFfAoqA84FzgTDwVVVNqepDwJqcz3g/8G1V/ZOqZlT1fiDhvW883gXcp6rPe/V9EjhPRGYDKaAMOB0QVd2kqnu996WAM0WkXFU7VPX5cX6uMQdZEJjJaH/OzwPDPC/1fp6O+wYOgKpmgSagwXtttx5+VcadOT/PAj7qDQt1ikgnMMN733gMraEX962/QVV/B3wTuBvYLyL3iki5t+oNwNXAThH5vYicN87PNeYgCwLjZ3twO3TAjcnjdua7gb1Ag7ds0Mycn5uAz6lqZc6jWFUfOM4aSnBDTbsBVPXrqroEWIAbIvq4t3yNql4HTMENYf14nJ9rzEEWBMbPfgxcIyKXiUgY+ChueOdp4BkgDfyDiIRE5K3A8pz3fge4VURe703qlojINSJSNs4a/gt4r4gs9uYXPo8bytohIsu87YeBPiAOZLw5jHeJSIU3pNUNZI7j92B8zoLA+JaqbgFuBr4BtOImlq9V1aSqJoG3Au8BOnDzCT/Jee9a3DzBN73Xt3rrjreG3wKfBh7G9UJOBW7yXi7HBU4HbvioDTePAfBuYIeIdAO3eu0w5piI3ZjGGGP8zXoExhjjcxYExhjjcxYExhjjcxYExhjjc6FCFzBetbW1Onv27EKXYYwxJ5XnnnuuVVXrhnvtpAuC2bNns3bt2kKXYYwxJxUR2TnSazY0ZIwxPmdBYIwxPpfXIBCRFSKyxbvW+h3DvF4hIj8TkRe8a8K/N5/1GGOMOVLe5ghEJIi7auLlQDOwRkRWqurGnNU+CGxU1WtFpA7YIiI/9E7vH7NUKkVzczPxeHzC6n+tisViNDY2Eg6HC12KMWaSyOdk8XJgq6puAxCRB4HrgNwgUKDMu8JjKdCOu9DXuDQ3N1NWVsbs2bM5/GKRk4uq0tbWRnNzM3PmzCl0OcaYSSKfQ0MNuEv1Dmr2luX6JnAG7lK8LwG3e9eEP4yI3CIia0VkbUtLyxEfFI/HqampmdQhACAi1NTU+KLnY4w5cfIZBMPtlYde4e5KYB3u5hyLgW/m3Hjj0JtU71XVpaq6tK5u2MNgJ30IDPJLO40xJ04+g6AZd5OPQY24b/653gv8RJ2twHbcbfkmXDyVYV9XnHTmiA6HMcb4Wj6DYA0wV0TmeDcCvwlYOWSdXcBlACJSD8wHtuWjmEQqw4GeOKnsxF92u7Ozk29961vjft/VV19NZ2fnhNdjjDHjkbcgUNU0cBvwK2AT8GNV3SAit4rIrd5qnwXOF5GXgN8Cn1DV1nzUIwE3pKInMAgymdFvGrVq1SoqKysnvB5jjBmPvF5iQlVXAauGLLsn5+c9wBX5rGFQwBtbz+bhRjx33HEHr776KosXLyYcDlNaWsq0adNYt24dGzdu5Prrr6epqYl4PM7tt9/OLbfcAhy6XEZvby9XXXUVF154IU8//TQNDQ08+uijFBUVTXitxhgz1El3raGj+ZefbWDjnu4jlmdVGUhmiIWDBAPjm3A9c3o5n7l2wYiv33XXXaxfv55169axevVqrrnmGtavX3/wEM/77ruP6upqBgYGWLZsGTfccAM1NTWHbeOVV17hgQce4Dvf+Q5vf/vbefjhh7n5Zrv7oDEm/yZdEBzNibgx5/Llyw87zv/rX/86jzzyCABNTU288sorRwTBnDlzWLx4MQBLlixhx44dJ6BSY4yZhEEw0jf3ZDrD5n09NFYVU10SyWsNJSUlB39evXo1jz/+OM888wzFxcVccsklw54HEI1GD/4cDAYZGBjIa43GGDPINxedGzz+XvMwR1BWVkZPT8+wr3V1dVFVVUVxcTGbN2/mj3/844R/vjHGHI9J1yMYyaHJ4onfdk1NDRdccAELFy6kqKiI+vr6g6+tWLGCe+65h0WLFjF//nzOPffciS/AGGOOg+TjG3I+LV26VIfemGbTpk2cccYZo75PVXlpdxf15THqy2P5LDHvxtJeY4zJJSLPqerS4V7z1dCQiOTl8FFjjDmZ+SYIAAIClgPGGHM4nwWB9QiMMWYonwVBfiaLjTHmZOarIBARspYExhhzGF8FgQ0NGWPMkXxzHgHkb7K4ra2Nyy67DIB9+/YRDAYZvIHOs88+SyQy+pnMq1evJhKJcP755098ccYYcxQ+CwIhlZ34G9PU1NSwbt06AO68805KS0v52Mc+Nub3r169mtLSUgsCY0xB+GpoSE7gZPFzzz3HxRdfzJIlS7jyyivZu3cv4C5Ad+aZZ7Jo0SJuuukmduzYwT333MNXvvIVFi9ezJNPPnliCjTGGM/k6xH88g7Y99KwL9WnM26yODLOZk89C666a8yrqyof+tCHePTRR6mrq+NHP/oRn/rUp7jvvvu466672L59O9FolM7OTiorK7n11lvH3YswxpiJMvmCYBTCibkMdSKRYP369Vx++eWAu1PZtGnTAFi0aBHvete7uP7667n++utPQDXGGDO6yRcEo3xzb+8aoK03ycKGiryWoKosWLCAZ5555ojXfvGLX/DEE0+wcuVKPvvZz7Jhw4a81mKMMUfjszkCd/hovi+0F41GaWlpORgEqVSKDRs2kM1maWpq4tJLL+ULX/gCnZ2d9Pb2jnoZa2OMyTdfBcHgHSrzfSpBIBDgoYce4hOf+ARnn302ixcv5umnnyaTyXDzzTdz1llncc455/CRj3yEyspKrr32Wh555BGbLDbGFMTkGxoaRe4N7AOM777FY3XnnXce/PmJJ5444vWnnnrqiGXz5s3jxRdfzEs9xhhzNL7sEdhVJowx5hCfBcGhHoExxhhn0gTBWCaA83nf4hPlZK7dGPPaNCmCIBaL0dbWdtSd5Mk+NKSqtLW1EYud3LfaNMa8tkyKyeLGxkaam5tpaWkZdb1EOktLT4JMe4RYOHiCqptYsViMxsbGQpdhjJlEJkUQhMNh5syZc9T11u/u4v0/fIp7372EK86YegIqM8aY175JMTQ0VkUR1wsYSGUKXIkxxrx2+CsIvOGguAWBMcYclNcgEJEVIrJFRLaKyB3DvP5xEVnnPdaLSEZEqvNVz2AQDCQtCIwxZlDegkBEgsDdwFXAmcA7ReTM3HVU9YuqulhVFwOfBH6vqu35qunQ0NDE35zGGGNOVvnsESwHtqrqNlVNAg8C142y/juBB/JYD9GQa67NERhjzCH5DIIGoCnnebO37AgiUgysAB4e4fVbRGStiKw92iGioxERisJBmyMwxpgc+QyC4a7qNtKpXNcCfxhpWEhV71XVpaq6dPCm8McqFg7YHIExxuTIZxA0AzNynjcCe0ZY9ybyPCw0qCgctKEhY4zJkc8gWAPMFZE5IhLB7exXDl1JRCqAi4FH81jLQbGIBYExxuTK25nFqpoWkduAXwFB4D5V3SAit3qv3+Ot+hbg16ral69achWFg8RtaMgYYw7K6yUmVHUVsGrIsnuGPP8+8P181pHLhoaMMeZwvjqzGNy5BBYExhhziO+CIBYO2lFDxhiTw3dBYOcRGGPM4XwZBDY0ZIwxh/gvCCI2NGSMMbl8FwSxcJC4XXTOGGMO8l0QFIWDJDNZ0hkLA2OMAT8GQcQ1OZ62IDDGGPBjENjNaYwx5jC+C4KY3a7SGGMO47sgsBvYG2PM4fwXBDY0ZIwxh/FPEGz5JfzbGVTF3U3TrEdgjDGOf4IAgZ49FGd7AAsCY4wZ5J8giFUAUJzpBbB7EhhjjMd3QRDzgsB6BMYY4/gnCIoqAYhlbGjIGGNy+ScIvB5BOO0FgQ0NGWMM4KcgCBdDIEQk6YLATigzxhjHP0EgArEKAokuggGxoSFjjPH4JwgAYhVIosvdnCZpF50zxhjwXRBUQrzL3bfYegTGGAP4LggqIN5FUSRgcwTGGOPxZxCE7XaVxhgzyL9BYD0CY4wB/BgEA502R2CMMTn8FwSZBOXhjM0RGGOMx19B4F1mojowYHMExhjjyWsQiMgKEdkiIltF5I4R1rlERNaJyAYR+X0+6yFWCUBVoN+GhowxxhPK14ZFJAjcDVwONANrRGSlqm7MWacS+BawQlV3iciUfNUDHLzeUEWgn3iqOK8fZYwxJ4t89giWA1tVdZuqJoEHgeuGrPOXwE9UdReAqh7IYz2HgkD6bWjIGGM8+QyCBqAp53mztyzXPKBKRFaLyHMi8lfDbUhEbhGRtSKytqWl5dgr8oKgnD4GUhlU9di3ZYwxk0Q+g0CGWTZ0zxsClgDXAFcCnxaReUe8SfVeVV2qqkvr6uqOvSJvjqCUPrIKyYxdb8gYY/I2R4DrAczIed4I7BlmnVZV7QP6ROQJ4Gzg5bxU5PUISrJ9AMSTWaKhYF4+yhhjThb57BGsAeaKyBwRiQA3ASuHrPMocJGIhESkGHg9sClvFYVjEIxSkrXbVRpjzKC89QhUNS0itwG/AoLAfaq6QURu9V6/R1U3ichjwItAFviuqq7PV00AxCoosiAwxpiD8jk0hKquAlYNWXbPkOdfBL6YzzoOE6s4dAN7O3LIGGN8dmYxQFEl0XQ3AAOpdIGLMcaYwvNfEMQqiKZdj6A7bkFgjDG+DIJI2t3Avq03WeBijDGm8HwZBMGkGxpq600UuBhjjCk8XwaBxLuIhoS2PusRGGOMD4OgEsmmaCgRWnusR2CMMT4MAnd28aziBK3WIzDGGP8GQUNRyuYIjDEGHwfBtGjCjhoyxhh8GQSVAEwJx2nrS9ilqI0xvue/IPDuW1wbjpPKKN0DdlKZMcbf/BcE3tBQdWAAgNY+mycwxvib/4IgWg5Ahbh7Etg8gTHG7/wXBKEIhIspYzAIrEdgjPE3/wUBQKyCYu+eBK0WBMYYnxtTEIjI7SJSLs5/iMjzInJFvovLm1jlwSuQttrQkDHG58baI/gbVe0GrgDqgPcCd+WtqnyLVRBIdFFVHKbNJouNMT431iAQ779XA99T1Rdylp18YhUQ76KmNGqTxcYY3xtrEDwnIr/GBcGvRKQMd4/hk1OsAuKd1JREbI7AGON7Y71n8fuAxcA2Ve0XkWrc8NDJyesR1NZF2bSnu9DVGGNMQY21R3AesEVVO0XkZuB/Al35KyvPBoOgOGw9AmOM7401CP4d6BeRs4F/BHYCP8hbVflWVAmaZWpRhu54mmT65B3lMsaY4zXWIEiruzrbdcDXVPVrQFn+ysoz7zITU6OuN2BHDhlj/GysQdAjIp8E3g38QkSCQDh/ZeWZFwRTInHALjNhjPG3sQbBO4AE7nyCfUAD8MW8VZVv3qWoawN2drExxowpCLyd/w+BChF5ExBX1ZN3jqCiEYDqdAtgPQJjjL+N9RITbweeBd4GvB34k4jcmM/C8qq8wf0nsRewOQJjjL+N9TyCTwHLVPUAgIjUAY8DD+WrsLwKx6C0nkjvbqKhxXa9IWOMr411jiAwGAKetnG897WpYgbS1URtadTmCIwxvjbWnfljIvIrEXmPiLwH+AWw6mhvEpEVIrJFRLaKyB3DvH6JiHSJyDrv8c/jK/84VM6AziZqSiM2R2CM8bUxDQ2p6sdF5AbgAtzF5u5V1UdGe493iOndwOVAM7BGRFaq6sYhqz6pqm8af+nHqWIGbF5FbUOIAzZHYIzxsbHOEaCqDwMPj2Pby4GtqroNQEQexJ2QNjQICqNyJmQSzI71s3GfFroaY4wpmFGHhkSkR0S6h3n0iMjRrtbWADTlPG/2lg11noi8ICK/FJEFI9Rxi4isFZG1LS0tR/nYMaqYAcCscDttfQncidPGGOM/o/YIVPV4LiMx3P0Khu5tnwdmqWqviFwN/BSYO0wd9wL3AixdunRi9tiVLggapZVUZgbd8TQVRSfvydLGGHOs8nnkTzMwI+d5I7AndwVV7VbVXu/nVUBYRGrzWNMhXo+gPrsfsJvYG2P8K59BsAaYKyJzRCQC3ASszF1BRKaKiHg/L/fqactjTYfEyiFWQXXKC4I+O3LIGONPY54sHi9VTYvIbcCvgCBwn6puEJFbvdfvAW4E/l5E0sAAcJOeyMH6ipmUJfYBsL87fsI+1hhjXkvyFgRwcLhn1ZBl9+T8/E3gm/msYVSVMyhu3w7Azrb+gpVhjDGFdHKfHXy8KmYQ7GqmrjTCzra+QldjjDEF4e8gqJwByR4WVGfZYT0CY4xP+TsIvCOHFpX2WI/AGONb/g4C71yC+bEO9ncn6E+mC1yQMcaceP4OgoqZAMwKtQM2YWyM8Sd/B0FJLYSKmKrushU2PGSM8SN/B4EIVDRSkXTnEtiEsTHGj/wdBACVMwj3NFNTYoeQGmP8yYKgwt2gZlZNMTtarUdgjPEfC4LKGdDfytzqIDusR2CM8SELAu/IoYUlPeztihNPZQpckDHGnFgWBN65BKdF3EVPd7Xb8JAxxl8sCGrnATAz426mtqPVhoeMMf5iQVBSC6X11PVvBeykMmOM/1gQANQvJNK6garisE0YG2N8x4IAYOpCaNnCKdVRCwJjjO9YEADUL4RMkmVlrXYugTHGdywIwAUBcHaomT1dAyTSdgipMcY/LAgAaudCMMJpugNVaGofKHRFxhhzwlgQAATDUDef+oHBI4dsnsAY4x8WBIPqz6K0czMAm/f1FLgYY4w5cSwIBk1dSKDvAOdUp3ixubPQ1RhjzAljQTCofgEAb6xp4cXmrgIXY4wxJ44FwaD6swBYFtvD3q44+7vjBS7IGGNODAuCQSU1UDaNU7PbAXihqbOw9RhjzAliQZCrfgFVPa8QDIgNDxljfMOCIFf9QgKtW1gwJcYLNmFsjPEJC4JcU8+CbIrL6rp4oakTVS10RcYYk3d5DQIRWSEiW0Rkq4jcMcp6y0QkIyI35rOeo/KOHHp9UTPd8TQ77JLUxhgfyFsQiEgQuBu4CjgTeKeInDnCev8H+FW+ahmz2nlQVMXpiRcB7HwCY4wv5LNHsBzYqqrbVDUJPAhcN8x6HwIeBg7ksZaxCQRhzhuo2PsHYmFhnR05ZIzxgXwGQQPQlPO82Vt2kIg0AG8B7sljHeMz52KkezdX1PfaIaTGGF/IZxDIMMuGzr5+FfiEqo563WcRuUVE1orI2paWlomqb3inXALAVcVb2LCnm1Qmm9/PM8aYAstnEDQDM3KeNwJ7hqyzFHhQRHYANwLfEpHrh25IVe9V1aWqurSuri5P5XqqT4GKmZydfoFEOssWuwCdMWaSy2cQrAHmisgcEYkANwErc1dQ1TmqOltVZwMPAR9Q1Z/msaajE4FTLmZq27MEyNo8gTFm0stbEKhqGrgNdzTQJuDHqrpBRG4VkVvz9bkT4pRLCCS6uLR8D6u35HkoyhhjCiyUz42r6ipg1ZBlw04Mq+p78lnLuMy5GIB31m7jtq0zGUhmKIoEC1yUMcbkh51ZPJzSOqhfyNLsC8RTWZ54xXoFxpjJy4JgJKdcQkXL89TFsvxm4/5CV2OMMXljQTCSUy5BMgn+ZuZ+frtpP2k7jNQYM0lZEIxk1vkQivGmwDN09Kd4bmdHoSsyxpi8sCAYSaQEzn4njU0/Y1qom1/b8JAxZpKyIBjNeR9EMgnuqHmK32zcb5elNsZMShYEo6mdC/Ou4sqBX7C/vZMt++0sY2PM5GNBcDTn30Ys2cENwSdZ9dK+QldjjDETzoLgaGZdANMWc1vRr3nwTztIpu3oIWPM5GJBcDQicN5tTE83cVb/H1n10t5CV2SMMRPKgmAsFlyPVs7i07Ef8YOnXrZJY2PMpGJBMBbBMHL1l5idbea8ff/F87vsnAJjzORhQTBW864gffqb+VD4EVb+7g+FrsYYYyaMBcE4hK7+AhIIc9n2L7C7o7/Q5RhjzISwIBiP8mn0X/RPvCHwIs+svLfQ1RhjzISwIBinqos/QHNsPhdu+xovN9l5BcaYk58FwXgFgpS/9ctMlXbWP/gZO4LIGHPSsyA4BuXzLmTH9DdxTe/DPPbkHwtdjjHGHBcLgmM08x1fJBsIEv3dP9PZnyx0OcYYc8wsCI5RoGI6Pctu5y94lh89+H0bIjLGnLQsCI7DlMv/Bx2xGbxn5z/xwg8/BelEoUsyxphxsyA4HuEYFR94nHUlF7B4690MfOM82PWnQldljDHjYkFwnALlUznjHx7iE0Wfob2rB73/zbDjqUKXZYwxY2ZBMAHKY2He/95buEk/z06tJfvDt0PTmkKXZYwxY2JBMEFOm1LKv73nMm7hn2lOlZL6wVthz7pCl2WMMUdlQTCBls+p5r4PXcuny/+VA8kw2XsvRf/zrfDSQ5AaKHR5xhgzLAuCCdZYVcy/33Ydd5/6bb6Rvp62nevh4ffBV8+Cdf8FWbvDmTHmtcWCIA+KIyE+9+43UnLlpzm3/8t8NPa/6C+ZAT/9e/jeCjeZ3L0XMqlCl2qMMYQKXcBkJSL87UWnsKixktv+K8aCXadwW9Uabt37A0q+f82hFStmwutvgSXvhWhp4Qo2xviW5POMWBFZAXwNCALfVdW7hrx+HfBZIAukgQ+r6qjHXi5dulTXrl2bp4rzo603wSN/3s3jm/azeUczr2c959ZnuXJ2kOmdz8H2J6CoCpbfAvOvgqmLIBA8tAFVd+9kY4w5RiLynKouHfa1fAWBiASBl4HLgWZgDfBOVd2Ys04p0KeqKiKLgB+r6umjbfdkDIJcnf1JHlzTxL1PbKO9L8lFc2t53+xWLtx7P6Gtv3IrxSph+jkQ74Su3dDXAtEyKK6B0npY8tew6CYI2MieMWZsChUE5wF3quqV3vNPAqjqv46y/n2qesZo2z3Zg2BQXyLN/c/s4D+f2cnerjiRUIDrTglwfdU2FqdfpKRtPZTUQnkDlE6BRC/0t8KBzXBgA0w7G678PEx/HWgGNAvR8kM9h2wWNj0KT34Z0nGYsRwal8Pcy6F8emEbb4w54QoVBDcCK1T1b73n7wZer6q3DVnvLcC/AlOAa1T1mWG2dQtwC8DMmTOX7Ny5My81F0I2qzy/q4Ofv7iXx9bvY193HIA5tSUsnVXF62ZVcc7MSk6rKyUUDLgd/PqH4fE7obv58I0VVUPjMhcSm3/hAqN2PlTNhuY1MNAOwSic9wG48CMQq3DvS3tXTw1Fxt+ATAp2/gFmXQhBm3Iy5rWqUEHwNuDKIUGwXFU/NML6bwD+WVXfONp2J0uPYDiqytYDvTzxSitPb23l+V0ddPS7I4uioQCnTytnwfRy5k0pZW51kAWtj1GqvYQGd8CtW6B5LbRshprT4OI7YOFb3XyDKrRsgae+Ai8+6IaZ5rzBLWt9GSIlcNFHYfnfQTh2qKh4F+x9Afb8GfrbYcl7oHqOe61rNzz0N9D0R5h7Bdz4PZvwNuY16qQYGvLW2Q4sU9XWkdaZzEEwlKqyo62fP+/qYMOebjbs6WLjnm664+nD1osEA5TFQsytL2XZ7Gpe3xhjXkMdtWVFBALDTDLvWed6FO2vwpQz3WPfS7D1N1DeCGfdAG2vwv710LHj0Psk6IaeXvdXMPtCWPVxSMVh8Tth7X1ukvsvfwxl9ZDsd4HUe8ANaQ10wsxzoWFJ/ie+E73Q9gpMW2yT7MZ4ChUEIdxk8WXAbtxk8V+q6oacdU4DXvUmi18H/Axo1FGK8lMQDEdVaelNsPVAL9ta+ugaSNETT9M1kGT9bhcWWe+3Fw4KUytiRIIBBpIZ+lMZymNh5k8t4/SpZZxaV8r0yiKmVcSoKokQ2PEE0f/+FwL7XkBqToX6hTB1oZu4nrbYDQM9+SV47n7IplyAvO1+qJsHWx6Dh97rhpui5W5HrMOcPDdtMSx7H1Q0Qs9+6N0PwbCbBC+pcz2ZiobD35OKuwlzzbpHpMStO3QnH++CZ++FZ77lhsFO/Qu49utQOSMffwozkpaX3e88XFToSkyOggSB98FXA1/FHT56n6p+TkRuBVDVe0TkE8BfASlgAPj4ZDx89ETqTaRZt6uT7W197OkcYE/nAOmMUhQJUhQO0t6XZPO+bra39h0MjMMpRYEs86ZXc87MKubVl5HKZImnMmRUqS6OMJ0DzGj7A82z3kJfNkwinaU8FmZa/yZmrfkcFFWRnbIQpi4gVjOLYGkNhEvc5PWz34WWTaM3orzBzXWEYrDvRTd8pZnD14lVuPmP8mmuB5DoduslumHulW5y/MkvgwTgsk9D5Sw3aZ4agIEO6G9zR2XFKqB0quvFVM12QRQpcZ+RGoCefW7SPlo2es19rdD0LDQ/63pDs86H2RdBSc3Y/nAng7ZXYcMjcM673e9rOGu/Bz//sPt9X/Nv7uCEE6VnH7z6O5i3Aoqrh18nm3U932iZ+1ISKZ7YGhK98N+fd8Ol1/871M2f2O0fh4IFQT5YEEyMeCpDc8cAe7tcWHT2pxABQWjrS/LnXR282NzFQCpz9I2NQgQqisJUl0SYVV3M3CmlLIvuoL44QFntdCrqGolKFvpc76CofQuB3Wvc1VuzKZh6lhtyqprlDU0FDu30W192PYpomeuFlDfA8vfD9MXuwzt2wKO3wY4nhyks4N6T6DkyZMqmudAY6Bhc2fV+Gpe44Ohv9x5trufR3+7+CxAIuQn5VJ97XjsfppwBdadDaR107oL27e69FY1QfYqbr2l9xQ3FdTbB/BWw7P2upzUe/e0u3AZFy922B3tOqq5N2YwLt9weVTrhXpOA+z0Hw+73KgK9LfD7/wPPfQ+yaXd484q74OybDt/Gs9+BVR9zc089+9zf58zr4dRL3fZzQ7i/zdVXv8A9imvc66k+CBVB7bxDNQ7WneiBSKkL6lD00GdnUvCnb8PquyDZ47Z7/ofg3A8cPmc10AGP3AovP+b9WYNQfyYU17p5tEDIHXBRPt09KmdBzSnupM9s2vVyD2yGTAIqZrheT2k9hItdLVt/Cz/7MHTtgqh3IMbbvgenXTa+v2OeWBCYY5LKZGnpSRANBYiFgwRE6OhP0tabpGsgRSwcoCgSJBoK0BNP09mfomsgRTKdJZnJkkhn6RpI0d6XoK03yfbWPra19pFMj3y9pVg4wPz6Ms6YVs6U8hjZrJJRRYBYOEgsHKAkGqK6OEJVSYTK4jDF4RCxSICSSIjiSBDJ3Tlls7B3HaBoMEo6GENj1RArJxAIEBJ1O9CevdC+Ddq2uv+GYm5nUDbVTYo3r3GPdNzttIqr3U5j8L+VM2HG610IBUJucn37E7D7OTdX0r4dUAhGXM+juAa6mt0DdT2m+gVu57f1ccgkYeb5bgfVvccFXlGV+5yKGe7ncJF7tG11tbVvO/IXGoq5gJQAdO+GVL+3vMhtK1LilvfuP/K9g2GZTrh6lrwHFr0dfvMZ94331Mvc8FvpFNe+1Z+H+VfD277v3v+Hr8MTX3Q7zkGBsPf7q3Hh2bN35H+AsUoXvL373e89VzDqeoMVM1zotL0Cp10O5/69m6/a/HP3GfOvglMudTU+epv7XV7xWaia435ne/7sAiabdo/Bfwu5Xw4C4UOHaI8kXOx+tzVz4c3fcCH/wE1wYBNc+GG3TuvL7stJotetm027LxgNr3O9k6JK9/cKRlwoJnrcl572be7fUMvLbj7ugttHrmMUFgTmNSOTVZra+9nXHaelJ0FLT4JUJktABBHY2xVn095uNu7tprM/RUAgFAiQVSU9/FjWYSKhADUlEcpjYVLZLImUC6R4KsNAKkNmyDZqSyM0VBXTWFVEXWmUquIIVSVhslmlL5mhL5GmJBpianmMqeVRYpEQmaySzmYpCgeZUh6jrjRKJHSUk/tSA24nUzb18LPGU3H37bhs2qETBHtb4Pn73TBMuNjNmZROdTvOzibXq0h0Q7LP7aBK691QWuNStx1w36LjnS5oune7nVh5o9uWBKGrye2UUv1up1Uxw+04B+dh0gn3GQOd7tvu8lugdq7bdjbjvv3/9+fcOoNOf5M7ciz3MOSBTkj2uuAJxw59ex7U3w77N7jthIvdI9HjduytL7ufy6ZC2XTXQ0kNuG/9Ax1up9612/UcL/qY2+kPbrt5LTz9Ddi2+lAvqbzBzWnNWDb63yqbceHTscMNh7W/6nbOdae7R7jI/f46d7m5q9SA+1uUTXU9ucGj7hI98PD74eVfui8HVXPcEXexCtdOcAdp7HvJtWEkEnDvrZsPZ90IC28Yvf6RNmNBYE42qooqhx31lPZ6Gb2JNG29STr6k3T2pw7u5HsTaTr6krT1JekeSBEOBYh6j6JwiKJIgFgoSCAgqBcs+7vjNLUP0NzRT1tfkp4hR2QFhBHmUg5XEgkSDgUIBQKURl1A1JfHqC4OEwoGCAWEQEDIqrqOQUCYXlnEjOpiplfECAUDCJBVpbU3yYGeOB19SRqqijh9ajnTKmIHezqqeqjXk0m5nUwhjo4aDJveFrdznrb48JB7Lchm3OHP+9e73kpJ7Yn9fFUXxKX1brhtOOmEG+pM9rmeTybpwiZS6npkFY2HH9J9jCwIjBmjpDecFQoIxdGgO+IqlWF/d4J9XXGSmazbqYvQn0xzoCfB/u443QNp0tksqYzSm0izvzvOge44Hf2pgz2IbBYQFy7pzNh6OINKoyGCAWEglSGZzhILB6goClNRFKayOEJNSYTqkgiRUIBs1m07Fg5S7S0PBwP0J9P0JzMEBGpKotSWRakqDlMcCVIUCVEUDhIUIRBwPatoaOSduqoST7k6cofiWnsTbN7bQzQcoL4sxpTyKLHwaywcfGq0ILBTQY3JEQkFqCuLHrasOBJiTm2IObUlE/Y52ayyvyfuTdjHyWaVrCri7aSnlEepLIqwq72fLfu6ebXFTT5Hw24HHU9l6OpP0TmQpKMvxcv7e2jvS5LOKsGAEBQhnsrQlzz2yf7yWIgp5TGqisNkFdJZJZnO0tGXpL0vSTKTpSQSZHplEXVlUXa09rGnK37EdgbPkl82p5rGqiKiITfX0z2Qprmjn+aOAfZ1xWnpdUOF4aBw+rRyzphWztTyGL0Jd4h0KqPUlkaoK4sytTzGzOpid7b9MJLpLB39SWpLowSHO5fGHMZ6BMZMYvFUxgVERimOBimJhEhns7T1JmntTdDZn6I/laE/kfYOEXYhFU9laO1N0NKboL0vSTAghAIBwsEAVcVhakqjlMVCtPYm2N0xwP6eBDOri1nUUMGC6eUHh932dsV5sbmLtTvb6ewffhxcBOpKo9SVucdAMsOmvUeeODlUJBhgTm0JM6qLSGeVRCpLXzLN3q44rb0JVA+tM6e2hNJYiLA3TNc5kKK151DbymIhymJhoqGAC9KAkMpkGUi6YcfiSIjGqiIaq4qIpzKsa+riheZO4skMCxrKObuxkvlTy6gri1JTEiWezvD7LS2s3nKAbS19LJldxUVz61g+u5pwSMhklWwWkpms+5xUhqb2fra19NHcMcCZ08q49PQpLGqsPBhkiXSGbBaKIsfWw7KhIWNMQWWzyrbWXlp7k8RTGeKpLKXREDOqi5hWUXTEZLuqsrtzgLbeJOVFYcpiIYIitPUlONCTYE9nnK0Hetl6oIfmjgFvKMsd3TatIsb0yiJqSiI0dw7w6oE+trf2Ek+5o9nSmSwVRS7MakoiZLJKTzxNdzxFKpP1ej9ZwsEAReEgsXCQvkSapvb+gz2sU+tKOLuxkuJokJd2d7NpTzfJzOFHFYnAOTMqmTuljDU729nm9epGUxIJMrUidvA8n8riMLFQkM6BJPFUlg9eeiofv3LUCzSPyIaGjDEFFQgIp00p47QpY1tfRGisKqax6vATvqpKIpw25Sgn9+WJqtLZnyIYFMpjh0/8JtNZmjr6D/a0VOG8U2uoLjl0BFVzRz8vNXehQEBcryMSChAOCtFQkBlVbohNROjsT/LEK6384ZVWFD04H7Rs9ggnyh0n6xEYY4wPjNYjsDubGGOMz1kQGGOMz1kQGGOMz1kQGGOMz1kQGGOMz1kQGGOMz1kQGGOMz1kQGGOMz510J5SJSAuw8xjfXgu0TmA5Jws/ttuPbQZ/ttuPbYbxt3uWqtYN98JJFwTHQ0TWjnRm3WTmx3b7sc3gz3b7sc0wse22oSFjjPE5CwJjjPE5vwXBvYUuoED82G4/thn82W4/thkmsN2+miMwxhhzJL/1CIwxxgxhQWCMMT7nmyAQkRUiskVEtorIHYWuJx9EZIaI/LeIbBKRDSJyu7e8WkR+IyKveP+tKnStE01EgiLyZxH5uffcD22uFJGHRGSz9zc/zyft/oj373u9iDwgIrHJ1m4RuU9EDojI+pxlI7ZRRD7p7du2iMiV4/08XwSBiASBu4GrgDOBd4rImYWtKi/SwEdV9QzgXOCDXjvvAH6rqnOB33rPJ5vbgU05z/3Q5q8Bj6nq6cDZuPZP6naLSAPwD8BSVV0IBIGbmHzt/j6wYsiyYdvo/T9+E7DAe8+3vH3emPkiCIDlwFZV3aaqSeBB4LoC1zThVHWvqj7v/dyD2zE04Np6v7fa/cD1BSkwT0SkEbgG+G7O4sne5nLgDcB/AKhqUlU7meTt9oSAIhEJAcXAHiZZu1X1CaB9yOKR2ngd8KCqJlR1O7AVt88bM78EQQPQlPO82Vs2aYnIbOAc4E9AvaruBRcWwBhvIX7S+Crwj0A2Z9lkb/MpQAvwPW9I7LsiUsIkb7eq7ga+BOwC9gJdqvprJnm7PSO18bj3b34JAhlm2aQ9blZESoGHgQ+raneh68knEXkTcEBVnyt0LSdYCHgd8O+qeg7Qx8k/HHJU3rj4dcAcYDpQIiI3F7aqgjvu/ZtfgqAZmJHzvBHXnZx0RCSMC4EfqupPvMX7RWSa9/o04ECh6suDC4A3i8gO3JDfX4jI/2Vytxncv+lmVf2T9/whXDBM9na/Ediuqi2qmgJ+ApzP5G83jNzG496/+SUI1gBzRWSOiERwEysrC1zThBMRwY0Zb1LVL+e8tBL4a+/nvwYePdG15YuqflJVG1V1Nu7v+jtVvZlJ3GYAVd0HNInIfG/RZcBGJnm7cUNC54pIsffv/TLcXNhkbzeM3MaVwE0iEhWROcBc4NlxbVlVffEArgZeBl4FPlXoevLUxgtxXcIXgXXe42qgBneUwSvef6sLXWue2n8J8HPv50nfZmAxsNb7e/8UqPJJu/8F2AysB/4TiE62dgMP4OZAUrhv/O8brY3Ap7x92xbgqvF+nl1iwhhjfM4vQ0PGGGNGYEFgjDE+Z0FgjDE+Z0FgjDE+Z0FgjDE+Z0FgTJ6JyCWDV0U15rXIgsAYY3zOgsAYj4jcLCLPisg6Efm2d4+DXhH5NxF5XkR+KyJ13rqLReSPIvKiiDwyeG14ETlNRB4XkRe895zqbb40594BP/TOikVE7hKRjd52vlSgphufsyAwBhCRM4B3ABeo6mIgA7wLKAGeV9XXAb8HPuO95QfAJ1R1EfBSzvIfAner6tm4a+Ds9ZafA3wYdz+MU4ALRKQaeAuwwNvO/85nG40ZiQWBMc5lwBJgjYis856fgru09Y+8df4vcKGIVACVqvp7b/n9wBtEpAxoUNVHAFQ1rqr93jrPqmqzqmZxl/6YDXQDceC7IvJWYHBdY04oCwJjHAHuV9XF3mO+qt45zHqjXZNluMsBD0rk/JwBQqqaxt1A5GHcTUYeG1/JxkwMCwJjnN8CN4rIFDh4f9hZuP9HbvTW+UvgKVXtAjpE5CJv+buB36u790OziFzvbSMqIsUjfaB334gKVV2FGzZaPOGtMmYMQoUuwJjXAlXdKCL/E/i1iARwV338IO6GLwtE5DmgCzePAO4ywPd4O/ptwHu95e8Gvi0i/8vbxttG+dgy4FERieF6Ex+Z4GYZMyZ29VFjRiEivapaWug6jMknGxoyxhifsx6BMcb4nPUIjDHG5ywIjDHG5ywIjDHG5ywIjDHG5ywIjDHG5/4/irDkzDWqV/EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize history for loss/ makes graph for train and test loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c55d2f-2ed3-4d74-9b31-95ac5d41b58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b04df-fca4-48a1-bf38-f6aa59d770c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b081a1c-b487-4ef5-ace8-1e3a4b7ba1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
